🧠 Artificial Neuron: Basic Concept
A neuron in a neural network receives inputs, processes them, and outputs a signal.

🧮 Equation:

z = w * x + b
w: weight (importance of the input)

x: input/factor

b: bias (shifts the activation)

z: weighted sum (input to activation function)

🔁 Activation Functions
These decide if a neuron should “fire” or not based on z.

✅ Sigmoid:

σ(z) = 1 / (1 + e^(-z))
Output range: (0, 1)

Useful when outputs need to be probabilities

Downsides: Not zero-centered, slow training (especially in deep networks)

✅ Tanh (Hyperbolic Tangent):

tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
Output range: (-1, 1)

Zero-centered → faster convergence during training

Similar shape to sigmoid, but more efficient in practice

✅ ReLU (Rectified Linear Unit):
ReLU(z) = max(0, z)
Output range: [0, ∞)

Fast computation, less likely to saturate

Best choice for hidden layers in deep neural networks

⚠️ Perceptron (Hard Threshold):
Outputs 0 or 1 (no gradient)

Not recommended for deep learning (cannot use backpropagation properly)

🧱 Neural Network Architecture
📥 Input Layer:
The first layer, takes raw data (e.g. pixels, numbers)

🔒 Hidden Layers:
Dense/Fully Connected Layers: Each neuron is connected to every neuron in the previous and next layer.

Non-linear transformations using activation functions.

The more layers, the "deeper" the network.

📤 Output Layer:
Produces final prediction (e.g. probability, category)

🔄 Forward Propagation
Data flows through layers

Activations are computed layer-by-layer

Final result is the prediction/output

🧮 Cost/Loss Function
Measures how wrong the prediction is.

📉 Mean Squared Error (MSE):

Cost = (1/n) * ∑(yᵢ - ŷᵢ)²
yᵢ: actual value

ŷᵢ: predicted value

n: number of examples

Used for regression tasks

Other common loss functions:

Cross-Entropy (for classification)

MAE (Mean Absolute Error)

🔁 Training Neural Networks
🔙 Backward Propagation:
Uses chain rule from calculus to calculate gradients

Propagates error back from output to earlier layers

⚙️ Gradient Descent:
Adjusts weights and biases to minimize the cost/loss.

Change = learning_rate × gradient

Learning Rate (α): How big the steps are (common values: 0.01 or 0.001)

⚡ Stochastic Gradient Descent (SGD):
Instead of using the full dataset at once:

Split data into mini-batches

Train on each batch

Faster, and uses less memory

📦 Batching
Batch Size = number of training examples used in one forward/backward pass (commonly 32)

Sweet spot: balances performance & computation

If training is too slow, reduce batch size (e.g., from 64 → 32 → 16)

🚀 Optimizers
Help improve convergence and speed of training.

SGD (Stochastic Gradient Descent): Simple, works well

Momentum: (Regular and Nesterov)

    - Use a Single Learning Rate

AdaGrad: Adapative Gradient - mimimises the need for tinkeing learning Rate

but the matrix is bigger it divides to the point learning rate short

AdaDelta: Resolves issues with AdaGrad in same manner as moment

RMSprop: Root Mean Squared Propagation

Adam: Adapative Moment Estimation (Extra Moment Average)

Ndam: RMSprop with Nesterov Momentum

Other advanced optimizers: Adam, RMSprop, AdaGrad (better for deep networks)

⚠️ Neuron Saturation
Occurs when activation outputs are stuck (very high/low values), causing gradients to vanish → leads to slow or no learning

Sigmoid and tanh are more prone to this issue

ReLU reduces this problem

🧠 Weight Regularization (a.k.a. Weight Cost)
Adds a penalty to large weights to prevent overfitting.

Two common types:

L1 Regularization: Adds absolute values of weights

L2 Regularization: Adds squared values of weights

Batch Normalisation:

2 learnable parameters

Helps to scale weights from overfitting

overfitting - Cost goes down, validation cost goes up

Reducing Cost

Exploding Gradient -> Inverse of Vanishing Gradient

Regularization Techniques

L1 Regularization: Adds absolute values of weights

L2 Regularization: Adds squared values of weights

Data Augmentation - Keras has a module for this

Dropout:

Apply to hyper parameter


--------------------------------------

Convolutional Neural Network

Artificial network that features Convolutional layers

Computer Vision

Convolutional layers extract features out of each part of image

Pooling layers


--------------------------------------------------------------
R-CNN 
YOLO

-----------------------------------------------------------

