w.x + b = z

w = weight
x = factor
b = bias

Activation: a 

Sigmoid - Neuron

sigma(z) = 1/1+e^-z

Cost = 1/n sum(y^i-y(hat)^i)^2

Natural exponential function

Artificial Neuron Function

Range: (0,1)

Tanh - Neuron

Similar to Sigmoid

Range: (-1,1)

Retified Linear Unit - (Relu)

Properties of biological Neuron

Relu = sigma(z) = max(0,z)

Non-Linear Architecture

Partial Calculus Calculation

Recommendations:

Relu - Best,  Efficient computations

tanh - Good, 0 centered output (Rapid training)

Sigmoid - Acceptable (Trains less rapidly), only situations where range is (0,1)

Perceptron - NOT RECOMMENDED FOR DEEP LEARNING MODELS


----------------------------------------------------------------------------------------------------

Hidden Layers In Neural Networks

- Dense layer (a.k.a. Fully-connected layer)
    Receives input from previous layer (Fully Connected)

- Layer Types
    Dense/softmax

- Input Layers

- Output Layers

- Hidden layer

- Forward Propgation

--------------------------------------------------------

Cost Functions

mean square error

Training Approach

- Backward Propgation

- Gradient Descent

Minimizing Cost

Learning Rate N (0.01, 0.001 is good to start with)

Memory/Compute power would hinder training process

Stochastic Gradient Descent

Split data into mini batches(sub-sets)

-------------------------------------------------

Batching

Batch size = 32 

Sweet spot 

If it too big decrease by ^2

If each epoch takes long with batch


-----------------------------------------------
Back Propgation

Chain rule from calculus

-------------------------------------------------
Optimizers

Stochastic Gradient Descent

-------------------------------------------------

Neuron Saturation deminishing

Weight Cost:

