w.x + b = z

w = weight
x = factor
b = bias

Activation: a 

Sigmoid - Neuron

sigma(z) = 1/1+e^-z

Natural exponential function

Artificial Neuron Function

Range: (0,1)

Tanh - Neuron

Similar to Sigmoid

Range: (-1,1)

Retified Linear Unit - (Relu)

Properties of biological Neuron

Relu = sigma(z) = max(0,z)

Non-Linear Architecture

Partial Calculus Calculation

Recommendations:

Relu - Best,  Efficient computations

tanh - Good, 0 centered output (Rapid training)

Sigmoid - Acceptable (Trains less rapidly), only situations where range is (0,1)

Perceptron - NOT RECOMMENDED FOR DEEP LEARNING MODELS


----------------------------------------------------------------------------------------------------

Hidden Layers In Neural Networks

- Dense layer (a.k.a. Fully-connected layer)
    Receives input from previous layer (Fully Connected)

- Layer Types
    Dense/softmax

- Input Layers

- Output Layers

- Hidden layer

- Forward Propgation

mean square error