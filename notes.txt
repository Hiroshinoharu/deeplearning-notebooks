ðŸ§  Artificial Neuron: Basic Concept
A neuron in a neural network receives inputs, processes them, and outputs a signal.

ðŸ§® Equation:

z = w * x + b
w: weight (importance of the input)

x: input/factor

b: bias (shifts the activation)

z: weighted sum (input to activation function)

ðŸ” Activation Functions
These decide if a neuron should â€œfireâ€ or not based on z.

âœ… Sigmoid:

Ïƒ(z) = 1 / (1 + e^(-z))
Output range: (0, 1)

Useful when outputs need to be probabilities

Downsides: Not zero-centered, slow training (especially in deep networks)

âœ… Tanh (Hyperbolic Tangent):

tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
Output range: (-1, 1)

Zero-centered â†’ faster convergence during training

Similar shape to sigmoid, but more efficient in practice

âœ… ReLU (Rectified Linear Unit):
ReLU(z) = max(0, z)
Output range: [0, âˆž)

Fast computation, less likely to saturate

Best choice for hidden layers in deep neural networks

âš ï¸ Perceptron (Hard Threshold):
Outputs 0 or 1 (no gradient)

Not recommended for deep learning (cannot use backpropagation properly)

ðŸ§± Neural Network Architecture
ðŸ“¥ Input Layer:
The first layer, takes raw data (e.g. pixels, numbers)

ðŸ”’ Hidden Layers:
Dense/Fully Connected Layers: Each neuron is connected to every neuron in the previous and next layer.

Non-linear transformations using activation functions.

The more layers, the "deeper" the network.

ðŸ“¤ Output Layer:
Produces final prediction (e.g. probability, category)

ðŸ”„ Forward Propagation
Data flows through layers

Activations are computed layer-by-layer

Final result is the prediction/output

ðŸ§® Cost/Loss Function
Measures how wrong the prediction is.

ðŸ“‰ Mean Squared Error (MSE):

Cost = (1/n) * âˆ‘(yáµ¢ - Å·áµ¢)Â²
yáµ¢: actual value

Å·áµ¢: predicted value

n: number of examples

Used for regression tasks

Other common loss functions:

Cross-Entropy (for classification)

MAE (Mean Absolute Error)

ðŸ” Training Neural Networks
ðŸ”™ Backward Propagation:
Uses chain rule from calculus to calculate gradients

Propagates error back from output to earlier layers

âš™ï¸ Gradient Descent:
Adjusts weights and biases to minimize the cost/loss.

Change = learning_rate Ã— gradient

Learning Rate (Î±): How big the steps are (common values: 0.01 or 0.001)

âš¡ Stochastic Gradient Descent (SGD):
Instead of using the full dataset at once:

Split data into mini-batches

Train on each batch

Faster, and uses less memory

ðŸ“¦ Batching
Batch Size = number of training examples used in one forward/backward pass (commonly 32)

Sweet spot: balances performance & computation

If training is too slow, reduce batch size (e.g., from 64 â†’ 32 â†’ 16)

ðŸš€ Optimizers
Help improve convergence and speed of training.

SGD (Stochastic Gradient Descent): Simple, works well

Momentum: (Regular and Nesterov)

    - Use a Single Learning Rate

AdaGrad: Adapative Gradient - mimimises the need for tinkeing learning Rate

but the matrix is bigger it divides to the point learning rate short

AdaDelta: Resolves issues with AdaGrad in same manner as moment

RMSprop: Root Mean Squared Propagation

Adam: Adapative Moment Estimation (Extra Moment Average)

Ndam: RMSprop with Nesterov Momentum

Other advanced optimizers: Adam, RMSprop, AdaGrad (better for deep networks)

âš ï¸ Neuron Saturation
Occurs when activation outputs are stuck (very high/low values), causing gradients to vanish â†’ leads to slow or no learning

Sigmoid and tanh are more prone to this issue

ReLU reduces this problem

ðŸ§  Weight Regularization (a.k.a. Weight Cost)
Adds a penalty to large weights to prevent overfitting.

Two common types:

L1 Regularization: Adds absolute values of weights

L2 Regularization: Adds squared values of weights

Batch Normalisation:

2 learnable parameters

Helps to scale weights from overfitting

overfitting - Cost goes down, validation cost goes up

Reducing Cost

Exploding Gradient -> Inverse of Vanishing Gradient

Regularization Techniques

L1 Regularization: Adds absolute values of weights

L2 Regularization: Adds squared values of weights

Data Augmentation - Keras has a module for this

Dropout:

Apply to hyper parameter


--------------------------------------

Convolutional Neural Network

Artificial network that features Convolutional layers

Computer Vision

Convolutional layers extract features out of each part of image

Pooling layers


--------------------------------------------------------------
R-CNN 
YOLO

-----------------------------------------------------------

