{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bab79f",
   "metadata": {},
   "source": [
    "# Shallow Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4271df6",
   "metadata": {},
   "source": [
    "In this notebook, we adapt our TensorFlow Shallow Net to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a9791",
   "metadata": {},
   "source": [
    "**Load dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6959f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9569940",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cba042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train = MNIST('data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test = MNIST('data', train=False, transform=transforms.ToTensor())\n",
    "# ... toTensor() converts the images to PyTorch tensors\n",
    "# ... and normalizes them to the range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fa2a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae405199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data[0] # not scaled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fcee52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.imshow(train.data[0].numpy().squeeze(), cmap='gray_r') # squeeze() removes the single-dimensional entries from the shape of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d591902f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
       "        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
       "        7, 8, 3, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.targets[0:100] # targets are the labels of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b3b7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694a5457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5feb3f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c734f08",
   "metadata": {},
   "source": [
    "**Batch data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6115d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "# ...DataLoader creates an iterable over the dataset, allowing us to iterate over the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0aa996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample, y_sample = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c67e2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a7ebd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9f0f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 0, 7, 8, 7, 1, 4, 6, 1, 1, 2, 0, 9, 7, 6, 7, 2, 8, 7, 8, 1, 7, 6,\n",
       "        2, 4, 9, 9, 4, 9, 6, 9, 6, 7, 1, 8, 3, 6, 3, 4, 7, 9, 1, 3, 6, 3, 1, 3,\n",
       "        1, 6, 1, 2, 5, 2, 5, 3, 4, 3, 8, 9, 6, 1, 6, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57de6cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0314, 0.2980, 0.7922, 0.9961, 0.9961, 0.9961,\n",
       "          1.0000, 0.9961, 0.9961, 0.9961, 0.9961, 0.7412, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4863, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9137, 0.8549, 0.4196, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0745, 0.9059, 0.9922, 0.9725, 0.7843, 0.7843, 0.7843,\n",
       "          0.6118, 0.3216, 0.3216, 0.1373, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.6941, 0.9922, 0.9922, 0.5529, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0980, 0.8745, 0.9922, 0.9725, 0.0824, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2824, 0.9922, 0.9922, 0.9804, 0.4196, 0.7098, 0.8824, 0.7412,\n",
       "          0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078,\n",
       "          0.7294, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.7843, 0.4902, 0.4667, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9020, 0.6824, 0.9020, 0.9020,\n",
       "          0.9020, 0.9843, 0.9922, 0.9412, 0.1961, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
       "          0.9922, 0.9922, 0.9922, 0.6000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5412, 0.9922, 0.9922, 0.6667, 0.0471, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
       "          0.9922, 0.9922, 0.6980, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.6471, 0.9922, 0.9922, 0.3843, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510,\n",
       "          0.5725, 0.6902, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.1569, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0549, 0.3765, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1529, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1569, 0.5451, 0.9922, 0.9922, 0.9922, 0.6510, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3255, 0.9451, 0.8784, 0.2784, 0.2588, 0.2588, 0.3647, 0.7176,\n",
       "          0.9490, 0.9922, 0.9922, 0.9922, 0.7373, 0.0784, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.7412, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.8275, 0.6118, 0.0745, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2314, 0.9216, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.8471, 0.3961, 0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.4353, 0.9412, 0.9922, 0.9922, 0.6980, 0.5294, 0.1725,\n",
       "          0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b5f68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat_sample = X_sample.view(X_sample.shape[0], -1) # view() reshapes the tensor to a 2D tensor with shape (batch_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "957d8db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c93eaf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.2980, 0.7922,\n",
       "        0.9961, 0.9961, 0.9961, 1.0000, 0.9961, 0.9961, 0.9961, 0.9961, 0.7412,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4863, 0.9922,\n",
       "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9137, 0.8549,\n",
       "        0.4196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.9059,\n",
       "        0.9922, 0.9725, 0.7843, 0.7843, 0.7843, 0.6118, 0.3216, 0.3216, 0.1373,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6941,\n",
       "        0.9922, 0.9922, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980,\n",
       "        0.8745, 0.9922, 0.9725, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2824, 0.9922, 0.9922, 0.9804, 0.4196, 0.7098, 0.8824, 0.7412, 0.0667,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0078, 0.7294, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "        0.7843, 0.4902, 0.4667, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2118, 0.9922, 0.9922, 0.9922, 0.9922, 0.9020, 0.6824, 0.9020,\n",
       "        0.9020, 0.9020, 0.9843, 0.9922, 0.9412, 0.1961, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2118, 0.9922, 0.9922, 0.9922, 0.6000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.5412, 0.9922, 0.9922, 0.6667, 0.0471, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2118, 0.9922, 0.9922, 0.6980, 0.0157, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6471, 0.9922, 0.9922, 0.3843,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0510, 0.5725, 0.6902, 0.4039, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.9922, 0.9922,\n",
       "        0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471, 0.9922,\n",
       "        0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471,\n",
       "        0.9922, 0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0549, 0.3765, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1529, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1569, 0.5451, 0.9922, 0.9922, 0.9922, 0.6510, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.3255, 0.9451, 0.8784, 0.2784, 0.2588, 0.2588, 0.3647,\n",
       "        0.7176, 0.9490, 0.9922, 0.9922, 0.9922, 0.7373, 0.0784, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.7412, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "        0.9922, 0.9922, 0.9922, 0.9922, 0.8275, 0.6118, 0.0745, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2314, 0.9216, 0.9922, 0.9922, 0.9922,\n",
       "        0.9922, 0.9922, 0.9922, 0.8471, 0.3961, 0.0667, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4353, 0.9412, 0.9922,\n",
       "        0.9922, 0.6980, 0.5294, 0.1725, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat_sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d913c3",
   "metadata": {},
   "source": [
    "**Design neural network architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d051be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_dense = 64\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "764e1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n_input, n_dense),  # hidden layer with input size n_input and output size n_dense\n",
    "    nn.Sigmoid(),  # Activation function\n",
    "    nn.Linear(n_dense, n_out)  # Fully connected layer with input size n_dense and output size n_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46fe9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]          50,240\n",
      "           Sigmoid-2                [-1, 1, 64]               0\n",
      "            Linear-3                [-1, 1, 10]             650\n",
      "================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, n_input))  # i is the batch size, n_input is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78646ca",
   "metadata": {},
   "source": [
    "**Configure training hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c89e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_fxn = nn.CrossEntropyLoss()  # CrossEntropyLoss combines softmax and negative log-likelihood loss in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent optimizer with learning Fashion MNISTYFFffffffffFFFFFFFFFFFFffffffffaaaaaaaaaalolrate 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e5496",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_pct(pred_y, true_y):\n",
    "    _, prediction = torch.max(pred_y, 1) # returns the maximum value and the index of the maximum value along the specified dimension\n",
    "    correct = (prediction == true_y).sum().item() # counts the number of correct predictions\n",
    "    return (correct / true_y.shape[0]) * 100  # returns the accuracy in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68fc532f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cd29d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs.\n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.241, Accuracy: 2.5% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.480, Accuracy: 5.6% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.715, Accuracy: 9.6% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.947, Accuracy: 15.4% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.175, Accuracy: 20.9% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.400, Accuracy: 26.6% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.619, Accuracy: 32.9% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.834, Accuracy: 39.3% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 2.044, Accuracy: 45.8% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.200, Accuracy: 6.9% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.395, Accuracy: 13.7% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.583, Accuracy: 20.7% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.763, Accuracy: 27.9% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.936, Accuracy: 35.0% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.103, Accuracy: 42.2% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.264, Accuracy: 49.6% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.419, Accuracy: 57.0% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.567, Accuracy: 64.6% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.140, Accuracy: 7.6% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.278, Accuracy: 15.3% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.408, Accuracy: 23.3% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.535, Accuracy: 31.2% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.657, Accuracy: 39.2% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.775, Accuracy: 47.5% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.890, Accuracy: 55.8% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 1.001, Accuracy: 64.0% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 1.109, Accuracy: 72.3% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.105, Accuracy: 8.4% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.205, Accuracy: 16.9% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.305, Accuracy: 25.4% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.402, Accuracy: 34.0% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.496, Accuracy: 42.6% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.588, Accuracy: 51.3% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.679, Accuracy: 59.9% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.766, Accuracy: 68.7% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.852, Accuracy: 77.4% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.085, Accuracy: 8.8% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.168, Accuracy: 17.6% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.247, Accuracy: 26.6% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.328, Accuracy: 35.4% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.405, Accuracy: 44.3% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.483, Accuracy: 53.2% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.558, Accuracy: 62.2% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.632, Accuracy: 71.1% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.706, Accuracy: 80.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.072, Accuracy: 9.0% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.142, Accuracy: 18.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.213, Accuracy: 27.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.283, Accuracy: 36.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.350, Accuracy: 45.2% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.417, Accuracy: 54.3% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.482, Accuracy: 63.5% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.549, Accuracy: 72.5% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.613, Accuracy: 81.7% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.063, Accuracy: 9.2% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.127, Accuracy: 18.3% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.188, Accuracy: 27.5% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.249, Accuracy: 36.7% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.312, Accuracy: 45.9% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.371, Accuracy: 55.1% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.430, Accuracy: 64.4% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.490, Accuracy: 73.6% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.548, Accuracy: 82.8% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.058, Accuracy: 9.3% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.115, Accuracy: 18.5% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.172, Accuracy: 27.9% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.229, Accuracy: 37.1% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.285, Accuracy: 46.4% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.340, Accuracy: 55.7% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.394, Accuracy: 65.1% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.448, Accuracy: 74.4% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.503, Accuracy: 83.7% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.052, Accuracy: 9.4% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.106, Accuracy: 18.7% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.158, Accuracy: 28.1% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.210, Accuracy: 37.5% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.262, Accuracy: 46.8% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.315, Accuracy: 56.2% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.367, Accuracy: 65.5% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.418, Accuracy: 74.9% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.467, Accuracy: 84.3% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.051, Accuracy: 9.3% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.098, Accuracy: 18.9% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.149, Accuracy: 28.2% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.198, Accuracy: 37.6% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.246, Accuracy: 47.1% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.296, Accuracy: 56.4% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.345, Accuracy: 65.8% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.392, Accuracy: 75.3% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.439, Accuracy: 84.7% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.047, Accuracy: 9.5% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.095, Accuracy: 18.9% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.141, Accuracy: 28.4% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.187, Accuracy: 37.8% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.233, Accuracy: 47.4% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.281, Accuracy: 56.8% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.327, Accuracy: 66.2% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.372, Accuracy: 75.7% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.418, Accuracy: 85.1% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.046, Accuracy: 9.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.091, Accuracy: 18.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.136, Accuracy: 28.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.180, Accuracy: 37.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.224, Accuracy: 47.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.267, Accuracy: 56.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.311, Accuracy: 66.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.355, Accuracy: 75.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.399, Accuracy: 85.5% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.043, Accuracy: 9.5% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.085, Accuracy: 19.1% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.127, Accuracy: 28.6% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.170, Accuracy: 38.2% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.212, Accuracy: 47.7% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.257, Accuracy: 57.2% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.299, Accuracy: 66.7% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.342, Accuracy: 76.2% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.385, Accuracy: 85.7% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.042, Accuracy: 9.5% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.084, Accuracy: 19.0% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.126, Accuracy: 28.6% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.168, Accuracy: 38.1% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.208, Accuracy: 47.7% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.250, Accuracy: 57.3% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.291, Accuracy: 66.8% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.332, Accuracy: 76.4% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.373, Accuracy: 85.9% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.041, Accuracy: 9.5% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.082, Accuracy: 19.1% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.122, Accuracy: 28.6% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.162, Accuracy: 38.2% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.202, Accuracy: 47.8% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.243, Accuracy: 57.3% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.284, Accuracy: 66.9% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.324, Accuracy: 76.5% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.362, Accuracy: 86.1% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.039, Accuracy: 9.6% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.077, Accuracy: 19.2% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.116, Accuracy: 28.8% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.155, Accuracy: 38.4% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.195, Accuracy: 48.0% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.234, Accuracy: 57.6% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.274, Accuracy: 67.2% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.312, Accuracy: 76.8% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.352, Accuracy: 86.4% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.037, Accuracy: 9.6% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.076, Accuracy: 19.2% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.114, Accuracy: 28.9% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.152, Accuracy: 38.5% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.191, Accuracy: 48.1% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.229, Accuracy: 57.7% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.268, Accuracy: 67.3% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.307, Accuracy: 76.9% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.344, Accuracy: 86.5% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.038, Accuracy: 9.6% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.076, Accuracy: 19.3% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.113, Accuracy: 28.9% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.150, Accuracy: 38.5% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.188, Accuracy: 48.2% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.226, Accuracy: 57.7% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.264, Accuracy: 67.3% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.301, Accuracy: 77.0% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.336, Accuracy: 86.7% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.036, Accuracy: 9.7% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.073, Accuracy: 19.4% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.110, Accuracy: 29.0% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.147, Accuracy: 38.6% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.186, Accuracy: 48.2% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.221, Accuracy: 57.9% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.257, Accuracy: 67.5% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.294, Accuracy: 77.2% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.331, Accuracy: 86.8% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.038, Accuracy: 9.6% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.075, Accuracy: 19.2% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.109, Accuracy: 29.0% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.146, Accuracy: 38.6% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.181, Accuracy: 48.3% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.218, Accuracy: 57.9% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.253, Accuracy: 67.6% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.287, Accuracy: 77.3% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.323, Accuracy: 86.9% \n",
      "\n",
      "Traing completed.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs.\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    avg_cost = 0.0\n",
    "    avg_accuracy = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader): #enumerate() returns the index and the value of the iterable\n",
    "        \n",
    "        # Forward Porgagation\n",
    "        X_flat = X.view(X.shape[0], -1)  # flatten the input\n",
    "        pred_y = model(X_flat) # forward pass through the model\n",
    "        cost = cost_fxn(pred_y, y) # compute the loss\n",
    "        avg_cost += cost/n_batches # accumulate the average cost\n",
    "        \n",
    "        # Backward Propagation and Optimization via gradiebnt descent\n",
    "        optimizer.zero_grad() # zero the gradients before the backward pass\n",
    "        cost.backward() # compute the gradients\n",
    "        optimizer.step() # update the weights\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_pct(pred_y, y) # compute the accuracy\n",
    "        avg_accuracy += accuracy/n_batches # accumulate the average accuracy\n",
    "        \n",
    "        if(i+1) % 100 == 0:\n",
    "            print('Epoch {}/{} complete. Cost: {:.3f}, Accuracy: {:.1f}% \\n'.format(epoch + 1, num_epochs, avg_cost, avg_accuracy))\n",
    "            \n",
    "print(\"Traing completed.\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65ac3e",
   "metadata": {},
   "source": [
    "**Test Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d5f9357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test_batches = len(test_loader)\n",
    "n_test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0832eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cost: 0.321, Test Accuracy: 91.1%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # no need to compute gradients during evaluation\n",
    "    \n",
    "    avg_test_cost = 0.0\n",
    "    avg_test_accuracy = 0.0\n",
    "    \n",
    "    for X,y in test_loader:\n",
    "        \n",
    "        # Make predictions\n",
    "        X_flat = X.view(X.shape[0], -1)\n",
    "        pred_y = model(X_flat)\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = cost_fxn(pred_y, y)\n",
    "        avg_test_cost += cost/n_test_batches\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        test_accuracy = accuracy_pct(pred_y, y)\n",
    "        avg_test_accuracy += test_accuracy/n_test_batches\n",
    "        \n",
    "    print('Test Cost: {:.3f}, Test Accuracy: {:.1f}%'.format(avg_test_cost, avg_test_accuracy))\n",
    "    \n",
    "# model.train() unodoes the eval() mode and sets the model back to training mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
