{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bab79f",
   "metadata": {},
   "source": [
    "# Shallow Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4271df6",
   "metadata": {},
   "source": [
    "In this notebook, we adapt our TensorFlow Shallow Net to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a9791",
   "metadata": {},
   "source": [
    "**Load dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6959f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9569940",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cba042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train = MNIST('data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test = MNIST('data', train=False, transform=transforms.ToTensor())\n",
    "# ... toTensor() converts the images to PyTorch tensors\n",
    "# ... and normalizes them to the range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fa2a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae405199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data[0] # not scaled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcee52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.imshow(train.data[0].numpy().squeeze(), cmap='gray') # squeeze() removes the single-dimensional entries from the shape of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d591902f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
       "        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
       "        7, 8, 3, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.targets[0:100] # targets are the labels of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b3b7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694a5457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5feb3f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c734f08",
   "metadata": {},
   "source": [
    "**Batch data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6115d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "# ...DataLoader creates an iterable over the dataset, allowing us to iterate over the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0aa996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample, y_sample = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c67e2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a7ebd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9f0f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 0, 7, 8, 7, 1, 4, 6, 1, 1, 2, 0, 9, 7, 6, 7, 2, 8, 7, 8, 1, 7, 6,\n",
       "        2, 4, 9, 9, 4, 9, 6, 9, 6, 7, 1, 8, 3, 6, 3, 4, 7, 9, 1, 3, 6, 3, 1, 3,\n",
       "        1, 6, 1, 2, 5, 2, 5, 3, 4, 3, 8, 9, 6, 1, 6, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57de6cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0314, 0.2980, 0.7922, 0.9961, 0.9961, 0.9961,\n",
       "          1.0000, 0.9961, 0.9961, 0.9961, 0.9961, 0.7412, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4863, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9137, 0.8549, 0.4196, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0745, 0.9059, 0.9922, 0.9725, 0.7843, 0.7843, 0.7843,\n",
       "          0.6118, 0.3216, 0.3216, 0.1373, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.6941, 0.9922, 0.9922, 0.5529, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0980, 0.8745, 0.9922, 0.9725, 0.0824, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2824, 0.9922, 0.9922, 0.9804, 0.4196, 0.7098, 0.8824, 0.7412,\n",
       "          0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078,\n",
       "          0.7294, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.7843, 0.4902, 0.4667, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9020, 0.6824, 0.9020, 0.9020,\n",
       "          0.9020, 0.9843, 0.9922, 0.9412, 0.1961, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
       "          0.9922, 0.9922, 0.9922, 0.6000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5412, 0.9922, 0.9922, 0.6667, 0.0471, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
       "          0.9922, 0.9922, 0.6980, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.6471, 0.9922, 0.9922, 0.3843, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510,\n",
       "          0.5725, 0.6902, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.1569, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0549, 0.3765, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1529, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1569, 0.5451, 0.9922, 0.9922, 0.9922, 0.6510, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3255, 0.9451, 0.8784, 0.2784, 0.2588, 0.2588, 0.3647, 0.7176,\n",
       "          0.9490, 0.9922, 0.9922, 0.9922, 0.7373, 0.0784, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.7412, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.8275, 0.6118, 0.0745, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2314, 0.9216, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.8471, 0.3961, 0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.4353, 0.9412, 0.9922, 0.9922, 0.6980, 0.5294, 0.1725,\n",
       "          0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b5f68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat_sample = X_sample.view(X_sample.shape[0], -1) # view() reshapes the tensor to a 2D tensor with shape (batch_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "957d8db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c93eaf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.2980, 0.7922,\n",
       "        0.9961, 0.9961, 0.9961, 1.0000, 0.9961, 0.9961, 0.9961, 0.9961, 0.7412,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4863, 0.9922,\n",
       "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9137, 0.8549,\n",
       "        0.4196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.9059,\n",
       "        0.9922, 0.9725, 0.7843, 0.7843, 0.7843, 0.6118, 0.3216, 0.3216, 0.1373,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6941,\n",
       "        0.9922, 0.9922, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980,\n",
       "        0.8745, 0.9922, 0.9725, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2824, 0.9922, 0.9922, 0.9804, 0.4196, 0.7098, 0.8824, 0.7412, 0.0667,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0078, 0.7294, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "        0.7843, 0.4902, 0.4667, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2118, 0.9922, 0.9922, 0.9922, 0.9922, 0.9020, 0.6824, 0.9020,\n",
       "        0.9020, 0.9020, 0.9843, 0.9922, 0.9412, 0.1961, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2118, 0.9922, 0.9922, 0.9922, 0.6000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.5412, 0.9922, 0.9922, 0.6667, 0.0471, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2118, 0.9922, 0.9922, 0.6980, 0.0157, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6471, 0.9922, 0.9922, 0.3843,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0510, 0.5725, 0.6902, 0.4039, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.9922, 0.9922,\n",
       "        0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471, 0.9922,\n",
       "        0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471,\n",
       "        0.9922, 0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0471, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0549, 0.3765, 0.9922, 0.9922, 0.7373, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1529, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1569, 0.5451, 0.9922, 0.9922, 0.9922, 0.6510, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.3255, 0.9451, 0.8784, 0.2784, 0.2588, 0.2588, 0.3647,\n",
       "        0.7176, 0.9490, 0.9922, 0.9922, 0.9922, 0.7373, 0.0784, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.7412, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "        0.9922, 0.9922, 0.9922, 0.9922, 0.8275, 0.6118, 0.0745, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2314, 0.9216, 0.9922, 0.9922, 0.9922,\n",
       "        0.9922, 0.9922, 0.9922, 0.8471, 0.3961, 0.0667, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4353, 0.9412, 0.9922,\n",
       "        0.9922, 0.6980, 0.5294, 0.1725, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat_sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d913c3",
   "metadata": {},
   "source": [
    "**Design neural network architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d051be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_dense = 64\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "764e1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n_input, n_dense),  # hidden layer with input size n_input and output size n_dense\n",
    "    nn.Sigmoid(),  # Activation function\n",
    "    nn.Linear(n_dense, n_out)  # Fully connected layer with input size n_dense and output size n_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46fe9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]          50,240\n",
      "           Sigmoid-2                [-1, 1, 64]               0\n",
      "            Linear-3                [-1, 1, 10]             650\n",
      "================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, n_input))  # i is the batch size, n_input is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78646ca",
   "metadata": {},
   "source": [
    "**Configure training hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c89e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_fxn = nn.CrossEntropyLoss()  # CrossEntropyLoss combines softmax and negative log-likelihood loss in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "462f82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Adam optimizer with learning rate 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e5496",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_pct(pred_y, true_y):\n",
    "    _, prediction = torch.max(pred_y, 1) # returns the maximum value and the index of the maximum value along the specified dimension\n",
    "    correct = (prediction == true_y).sum().item() # counts the number of correct predictions\n",
    "    return (correct / true_y.shape[0]) * 100  # returns the accuracy in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68fc532f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cd29d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs.\n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.241, Accuracy: 2.5% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.480, Accuracy: 5.6% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.715, Accuracy: 9.6% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 0.947, Accuracy: 15.4% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.175, Accuracy: 20.9% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.400, Accuracy: 26.6% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.619, Accuracy: 32.9% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 1.834, Accuracy: 39.3% \n",
      "\n",
      "Epoch 1/20 complete. Cost: 2.044, Accuracy: 45.8% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.200, Accuracy: 6.9% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.395, Accuracy: 13.7% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.583, Accuracy: 20.7% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.763, Accuracy: 27.9% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 0.936, Accuracy: 35.0% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.103, Accuracy: 42.2% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.264, Accuracy: 49.6% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.419, Accuracy: 57.0% \n",
      "\n",
      "Epoch 2/20 complete. Cost: 1.567, Accuracy: 64.6% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.140, Accuracy: 7.6% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.278, Accuracy: 15.3% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.408, Accuracy: 23.3% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.535, Accuracy: 31.2% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.657, Accuracy: 39.2% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.775, Accuracy: 47.5% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 0.890, Accuracy: 55.8% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 1.001, Accuracy: 64.0% \n",
      "\n",
      "Epoch 3/20 complete. Cost: 1.109, Accuracy: 72.3% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.105, Accuracy: 8.4% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.205, Accuracy: 16.9% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.305, Accuracy: 25.4% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.402, Accuracy: 34.0% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.496, Accuracy: 42.6% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.588, Accuracy: 51.3% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.679, Accuracy: 59.9% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.766, Accuracy: 68.7% \n",
      "\n",
      "Epoch 4/20 complete. Cost: 0.852, Accuracy: 77.4% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.085, Accuracy: 8.8% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.168, Accuracy: 17.6% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.247, Accuracy: 26.6% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.328, Accuracy: 35.4% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.405, Accuracy: 44.3% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.483, Accuracy: 53.2% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.558, Accuracy: 62.2% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.632, Accuracy: 71.1% \n",
      "\n",
      "Epoch 5/20 complete. Cost: 0.706, Accuracy: 80.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.072, Accuracy: 9.0% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.142, Accuracy: 18.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.213, Accuracy: 27.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.283, Accuracy: 36.1% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.350, Accuracy: 45.2% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.417, Accuracy: 54.3% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.482, Accuracy: 63.5% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.549, Accuracy: 72.5% \n",
      "\n",
      "Epoch 6/20 complete. Cost: 0.613, Accuracy: 81.7% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.063, Accuracy: 9.2% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.127, Accuracy: 18.3% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.188, Accuracy: 27.5% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.249, Accuracy: 36.7% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.312, Accuracy: 45.9% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.371, Accuracy: 55.1% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.430, Accuracy: 64.4% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.490, Accuracy: 73.6% \n",
      "\n",
      "Epoch 7/20 complete. Cost: 0.548, Accuracy: 82.8% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.058, Accuracy: 9.3% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.115, Accuracy: 18.5% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.172, Accuracy: 27.9% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.229, Accuracy: 37.1% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.285, Accuracy: 46.4% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.340, Accuracy: 55.7% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.394, Accuracy: 65.1% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.448, Accuracy: 74.4% \n",
      "\n",
      "Epoch 8/20 complete. Cost: 0.503, Accuracy: 83.7% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.052, Accuracy: 9.4% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.106, Accuracy: 18.7% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.158, Accuracy: 28.1% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.210, Accuracy: 37.5% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.262, Accuracy: 46.8% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.315, Accuracy: 56.2% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.367, Accuracy: 65.5% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.418, Accuracy: 74.9% \n",
      "\n",
      "Epoch 9/20 complete. Cost: 0.467, Accuracy: 84.3% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.051, Accuracy: 9.3% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.098, Accuracy: 18.9% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.149, Accuracy: 28.2% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.198, Accuracy: 37.6% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.246, Accuracy: 47.1% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.296, Accuracy: 56.4% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.345, Accuracy: 65.8% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.392, Accuracy: 75.3% \n",
      "\n",
      "Epoch 10/20 complete. Cost: 0.439, Accuracy: 84.7% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.047, Accuracy: 9.5% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.095, Accuracy: 18.9% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.141, Accuracy: 28.4% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.187, Accuracy: 37.8% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.233, Accuracy: 47.4% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.281, Accuracy: 56.8% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.327, Accuracy: 66.2% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.372, Accuracy: 75.7% \n",
      "\n",
      "Epoch 11/20 complete. Cost: 0.418, Accuracy: 85.1% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.046, Accuracy: 9.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.091, Accuracy: 18.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.136, Accuracy: 28.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.180, Accuracy: 37.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.224, Accuracy: 47.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.267, Accuracy: 56.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.311, Accuracy: 66.4% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.355, Accuracy: 75.9% \n",
      "\n",
      "Epoch 12/20 complete. Cost: 0.399, Accuracy: 85.5% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.043, Accuracy: 9.5% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.085, Accuracy: 19.1% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.127, Accuracy: 28.6% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.170, Accuracy: 38.2% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.212, Accuracy: 47.7% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.257, Accuracy: 57.2% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.299, Accuracy: 66.7% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.342, Accuracy: 76.2% \n",
      "\n",
      "Epoch 13/20 complete. Cost: 0.385, Accuracy: 85.7% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.042, Accuracy: 9.5% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.084, Accuracy: 19.0% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.126, Accuracy: 28.6% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.168, Accuracy: 38.1% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.208, Accuracy: 47.7% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.250, Accuracy: 57.3% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.291, Accuracy: 66.8% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.332, Accuracy: 76.4% \n",
      "\n",
      "Epoch 14/20 complete. Cost: 0.373, Accuracy: 85.9% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.041, Accuracy: 9.5% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.082, Accuracy: 19.1% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.122, Accuracy: 28.6% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.162, Accuracy: 38.2% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.202, Accuracy: 47.8% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.243, Accuracy: 57.3% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.284, Accuracy: 66.9% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.324, Accuracy: 76.5% \n",
      "\n",
      "Epoch 15/20 complete. Cost: 0.362, Accuracy: 86.1% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.039, Accuracy: 9.6% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.077, Accuracy: 19.2% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.116, Accuracy: 28.8% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.155, Accuracy: 38.4% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.195, Accuracy: 48.0% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.234, Accuracy: 57.6% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.274, Accuracy: 67.2% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.312, Accuracy: 76.8% \n",
      "\n",
      "Epoch 16/20 complete. Cost: 0.352, Accuracy: 86.4% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.037, Accuracy: 9.6% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.076, Accuracy: 19.2% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.114, Accuracy: 28.9% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.152, Accuracy: 38.5% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.191, Accuracy: 48.1% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.229, Accuracy: 57.7% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.268, Accuracy: 67.3% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.307, Accuracy: 76.9% \n",
      "\n",
      "Epoch 17/20 complete. Cost: 0.344, Accuracy: 86.5% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.038, Accuracy: 9.6% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.076, Accuracy: 19.3% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.113, Accuracy: 28.9% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.150, Accuracy: 38.5% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.188, Accuracy: 48.2% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.226, Accuracy: 57.7% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.264, Accuracy: 67.3% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.301, Accuracy: 77.0% \n",
      "\n",
      "Epoch 18/20 complete. Cost: 0.336, Accuracy: 86.7% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.036, Accuracy: 9.7% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.073, Accuracy: 19.4% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.110, Accuracy: 29.0% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.147, Accuracy: 38.6% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.186, Accuracy: 48.2% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.221, Accuracy: 57.9% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.257, Accuracy: 67.5% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.294, Accuracy: 77.2% \n",
      "\n",
      "Epoch 19/20 complete. Cost: 0.331, Accuracy: 86.8% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.038, Accuracy: 9.6% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.075, Accuracy: 19.2% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.109, Accuracy: 29.0% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.146, Accuracy: 38.6% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.181, Accuracy: 48.3% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.218, Accuracy: 57.9% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.253, Accuracy: 67.6% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.287, Accuracy: 77.3% \n",
      "\n",
      "Epoch 20/20 complete. Cost: 0.323, Accuracy: 86.9% \n",
      "\n",
      "Traing completed.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs.\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    avg_cost = 0.0\n",
    "    avg_accuracy = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader): #enumerate() returns the index and the value of the iterable\n",
    "        \n",
    "        # Forward Porgagation\n",
    "        X_flat = X.view(X.shape[0], -1)  # flatten the input\n",
    "        pred_y = model(X_flat) # forward pass through the model\n",
    "        cost = cost_fxn(pred_y, y) # compute the loss\n",
    "        avg_cost += cost/n_batches # accumulate the average cost\n",
    "        \n",
    "        # Backward Propagation and Optimization via gradiebnt descent\n",
    "        optimizer.zero_grad() # zero the gradients before the backward pass\n",
    "        cost.backward() # compute the gradients\n",
    "        optimizer.step() # update the weights\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_pct(pred_y, y) # compute the accuracy\n",
    "        avg_accuracy += accuracy/n_batches # accumulate the average accuracy\n",
    "        \n",
    "        if(i+1) % 100 == 0:\n",
    "            print('Epoch {}/{} complete. Cost: {:.3f}, Accuracy: {:.1f}% \\n'.format(epoch + 1, num_epochs, avg_cost, avg_accuracy))\n",
    "            \n",
    "print(\"Traing completed.\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65ac3e",
   "metadata": {},
   "source": [
    "**Test Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d5f9357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test_batches = len(test_loader)\n",
    "n_test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac0832eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cost: 0.321, Test Accuracy: 91.1%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # no need to compute gradients during evaluation\n",
    "    \n",
    "    avg_test_cost = 0.0\n",
    "    avg_test_accuracy = 0.0\n",
    "    \n",
    "    for X,y in test_loader:\n",
    "        \n",
    "        # Make predictions\n",
    "        X_flat = X.view(X.shape[0], -1)\n",
    "        pred_y = model(X_flat)\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = cost_fxn(pred_y, y)\n",
    "        avg_test_cost += cost/n_test_batches\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        test_accuracy = accuracy_pct(pred_y, y)\n",
    "        avg_test_accuracy += test_accuracy/n_test_batches\n",
    "        \n",
    "    print('Test Cost: {:.3f}, Test Accuracy: {:.1f}%'.format(avg_test_cost, avg_test_accuracy))\n",
    "    \n",
    "# model.train() unodoes the eval() mode and sets the model back to training mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
